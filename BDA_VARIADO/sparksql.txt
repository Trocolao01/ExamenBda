1. Registrar los DataFrames como tablas temporales en PySpark

Primero, aseguramos que ambos DataFrames (sales y stores) estén disponibles como tablas SQL en PySpark.

# Registrar los DataFrames como vistas temporales
df_sales.createOrReplaceTempView("sales")
df_stores.createOrReplaceTempView("stores")

Esto permite hacer consultas SQL sobre estos DataFrames a través de PySpark SQL.
2. Consultas SQL

Ahora, realizamos las consultas que mencionas. Utilizando spark.sql() ejecutamos las consultas SQL sobre las vistas temporales creadas.
Análisis de ventas:
a) ¿Qué tienda tiene los mayores ingresos totales?

query_ventas = """
SELECT store_id, SUM(revenue) AS total_revenue
FROM sales
GROUP BY store_id
ORDER BY total_revenue DESC
LIMIT 1
"""
resultado_ventas = spark.sql(query_ventas)
resultado_ventas.show()

b) ¿Cuáles son los ingresos totales generados en una fecha concreta?

fecha = "2025-04-01"  # Puedes cambiar la fecha a la que quieras consultar
query_ingresos_fecha = f"""
SELECT SUM(revenue) AS total_revenue
FROM sales
WHERE date = '{fecha}'
"""
resultado_ingresos_fecha = spark.sql(query_ingresos_fecha)
resultado_ingresos_fecha.show()

c) ¿Qué producto tiene la mayor cantidad vendida?

query_producto_mas_vendido = """
SELECT product_id, SUM(quantity_sold) AS total_quantity_sold
FROM sales
GROUP BY product_id
ORDER BY total_quantity_sold DESC
LIMIT 1
"""
resultado_producto_mas_vendido = spark.sql(query_producto_mas_vendido)
resultado_producto_mas_vendido.show()

Análisis geográfico:
a) ¿Cuáles son las regiones con mejores resultados en función de los ingresos?

query_regiones_mejores_resultados = """
SELECT s.location, SUM(sa.revenue) AS total_revenue
FROM sales sa
JOIN stores s ON sa.store_id = s.store_id
GROUP BY s.location
ORDER BY total_revenue DESC
"""
resultado_regiones_mejores_resultados = spark.sql(query_regiones_mejores_resultados)
resultado_regiones_mejores_resultados.show()

b) ¿Existe alguna correlación entre la ubicación de la tienda y el rendimiento de las ventas?

Aquí, podemos calcular la correlación entre la ubicación y los ingresos utilizando el CORR de SQL para obtener una métrica de correlación, aunque los ingresos son una métrica numérica y la ubicación es una cadena. Se podría hacer una conversión de ubicación a un índice numérico.

query_correlacion_ubicacion_ingresos = """
SELECT location, SUM(sa.revenue) AS total_revenue
FROM sales sa
JOIN stores s ON sa.store_id = s.store_id
GROUP BY location
"""
resultado_correlacion_ubicacion_ingresos = spark.sql(query_correlacion_ubicacion_ingresos)
resultado_correlacion_ubicacion_ingresos.show()

Para una verdadera correlación, deberías convertir location en un índice numérico, lo cual se puede hacer mediante codificación (por ejemplo, usando StringIndexer).
Análisis demográfico:
a) ¿Cómo varía el rendimiento de las ventas entre los distintos grupos demográficos?

query_ventas_demograficas = """
SELECT s.demographics, SUM(sa.revenue) AS total_revenue
FROM sales sa
JOIN stores s ON sa.store_id = s.store_id
GROUP BY s.demographics
ORDER BY total_revenue DESC
"""
resultado_ventas_demograficas = spark.sql(query_ventas_demograficas)
resultado_ventas_demograficas.show()

b) ¿Existen productos específicos preferidos por determinados grupos demográficos?

query_productos_por_demografia = """
SELECT s.demographics, sa.product_id, SUM(sa.quantity_sold) AS total_quantity_sold
FROM sales sa
JOIN stores s ON sa.store_id = s.store_id
GROUP BY s.demographics, sa.product_id
ORDER BY total_quantity_sold DESC
"""
resultado_productos_por_demografia = spark.sql(query_productos_por_demografia)
resultado_productos_por_demografia.show()

Análisis temporal:
a) ¿Cómo varía el rendimiento de las ventas a lo largo del tiempo (diariamente, semanalmente, mensualmente)?

query_ventas_temporal = """
SELECT date, SUM(revenue) AS total_revenue
FROM sales
GROUP BY date
ORDER BY date
"""
resultado_ventas_temporal = spark.sql(query_ventas_temporal)
resultado_ventas_temporal.show()

b) ¿Existen tendencias estacionales en las ventas?

Para detectar tendencias estacionales, podemos agrupar las ventas por mes o por trimestre.

query_ventas_estacionales = """
SELECT MONTH(date) AS month, SUM(revenue) AS total_revenue
FROM sales
GROUP BY month
ORDER BY month
"""
resultado_ventas_estacionales = spark.sql(query_ventas_estacionales)
resultado_ventas_estacionales.show()



df_sales.groupBy("store_id").count().orderBy("count", ascending=False).show()
df_sales.filter(df_sales["product_id"] == "12345").count()