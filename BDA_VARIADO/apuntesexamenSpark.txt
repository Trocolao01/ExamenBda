docker para lanzar el contenedor:


docker exec -it postgres-db psql -U postgres -d retail_db


 1. Ver las primeras filas del DataFrame

df.show(5)  # Muestra las primeras 5 filas

ðŸ“Œ Muestra los datos en formato de tabla.
ðŸ”¹ 2. Ver la estructura del DataFrame

df.printSchema()

ðŸ“Œ Muestra las columnas y sus tipos de datos.
ðŸ”¹ 3. Contar el nÃºmero de filas

df.count()

ðŸ“Œ Devuelve el nÃºmero total de registros en el DataFrame.
ðŸ”¹ 4. Mostrar nombres de columnas

df.columns

ðŸ“Œ Devuelve una lista con los nombres de las columnas.
ðŸ”¹ 5. Describir estadÃ­sticas bÃ¡sicas de las columnas numÃ©ricas

df.describe().show()

ðŸ“Œ Muestra la media, desviaciÃ³n estÃ¡ndar, mÃ­nimo y mÃ¡ximo de cada columna numÃ©rica.
ðŸ”¹ 6. Seleccionar columnas especÃ­ficas

df.select("columna1", "columna2").show()

ðŸ“Œ Filtra solo las columnas deseadas.
ðŸ”¹ 7. Filtrar datos con condiciones

df.filter(df["columna1"] > 100).show()

ðŸ“Œ Filtra las filas donde columna1 es mayor que 100.

df.filter(df["columna2"] == "valor").show()

ðŸ“Œ Filtra filas donde columna2 sea igual a "valor".
ðŸ”¹ 8. Ordenar el DataFrame

df.orderBy("columna1").show()

ðŸ“Œ Ordena los datos por columna1 de menor a mayor.

df.orderBy(df["columna1"].desc()).show()

ðŸ“Œ Ordena los datos en orden descendente.
ðŸ”¹ 9. Crear una nueva columna derivada

from pyspark.sql.functions import col

df = df.withColumn("nueva_columna", col("columna1") * 2)
df.show()

ðŸ“Œ Crea una columna nueva basada en cÃ¡lculos con otra columna.
ðŸ”¹ 10. Renombrar una columna

df = df.withColumnRenamed("columna1", "nueva_columna1")
df.show()

ðŸ“Œ Cambia el nombre de una columna.
ðŸ”¹ 11. Eliminar columnas

df = df.drop("columna_a_eliminar")
df.show()

ðŸ“Œ Elimina la columna indicada.
ðŸ”¹ 12. Eliminar filas duplicadas

df = df.dropDuplicates()
df.show()

ðŸ“Œ Elimina filas duplicadas en todas las columnas.

df = df.dropDuplicates(["columna1"])
df.show()

ðŸ“Œ Elimina filas duplicadas basadas solo en columna1.
ðŸ”¹ 13. Manejar valores nulos

ðŸ”¹ Eliminar filas con valores nulos:

df = df.na.drop()

ðŸ“Œ Elimina todas las filas que contengan valores nulos.

ðŸ”¹ Rellenar valores nulos con un valor especÃ­fico:

df = df.na.fill(0, subset=["columna1"])

ðŸ“Œ Rellena los valores nulos de columna1 con 0.

ðŸ”¹ Rellenar valores nulos con la media:

from pyspark.sql.functions import mean

media_valor = df.select(mean("columna1")).collect()[0][0]
df = df.na.fill(media_valor, subset=["columna1"])

ðŸ“Œ Calcula la media de columna1 y reemplaza los valores nulos con ella.
ðŸ”¹ 14. Agrupar y realizar agregaciones

df.groupBy("columna1").count().show()

ðŸ“Œ Cuenta cuÃ¡ntos registros hay por cada valor Ãºnico en columna1.

df.groupBy("columna1").agg({"columna2": "avg"}).show()

ðŸ“Œ Agrupa por columna1 y calcula la media de columna2.
ðŸ”¹ 15. Convertir un DataFrame de PySpark a Pandas

df_pandas = df.toPandas()

ðŸ“Œ Convierte el DataFrame de PySpark en un DataFrame de Pandas.


awslocal s3 ls s3://sample-bucket


def verificar_columnas_vacias(df):
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        empty_count = df.filter(col(column) == "ERROR").count()  # Para cadenas vacÃ­as
        print(f"Columna: {column}")
        print(f"  Valores nulos: {null_count}")
        print(f"  Valores vacÃ­os: {empty_count}")
        print("")
		
		
		
df_csv = df_csv.filter(
    (col("store_id").cast("string").rlike("^[0-9]+$")) &  
    (col("product_id").rlike("^[a-zA-Z0-9]+$"))         
)
df_kafka = df_kafka.filter(
    (col("store_id").cast("string").rlike("^[0-9]+$")) &  
    (col("product_id").rlike("^[a-zA-Z0-9]+$"))         
)
df_postgres = df_postgres.filter(
    (col("store_id").cast("string").rlike("^[0-9]+$"))   
)


def tratar_columnas_no_numericas(df, columnas):
    
    for columna in columnas:
        # Calcular la media de la columna
        mean_value = df.select(mean(col(columna))).collect()[0][0]
        
        # Reemplazar los valores no numÃ©ricos o incorrectos por la media
        df = df.withColumn(
            columna,
            when(
                (col(columna).cast(DoubleType()).isNotNull()) | (col(columna).cast(IntegerType()).isNotNull()), 
                col(columna)
            ).otherwise(mean_value)
        )
        
        # AÃ±adir la columna 'Tratados' con 'SÃ­' para los valores tratados y 'No' para los demÃ¡s
        df = df.withColumn(
            "Tratados",
            when(col(columna) != mean_value, "SÃ­").otherwise("No")
        )
    
    # AÃ±adir la columna 'Fecha InserciÃ³n' con la fecha actual en formato UTC
    df = df.withColumn("Fecha InserciÃ³n", current_timestamp())
    
    return df
	
	
	df_csv.select("date").distinct().show(10)  # Muestra los valores Ãºnicos en la columna `date`
	
	
	
	
docker exec -it postgres psql -U postgres -d retail_db -c "SELECT * FROM stores LIMIT 10;"


docker exec -it spark-master bash -c "python /apps_bda/data_integration.py"


docker cp apps_bda\data_integration.py spark-master:/data_integration.py


docker cp "apps_bda/data_integration.py" spark-master:/apps_bda/data_integration.py


awslocal s3 ls


awslocal s3 ls s3://sample-bucket

awslocal s3 ls s3://sample-bucket/output/part-00000-9776b7da-3caa-49e2-9bf9-4669bb951d50-c000.csv

cantidad_tienda_1 = df.filter(df.tienda == "Tienda 1").count()

print(f"Cantidad de veces que aparece 'Tienda 1': {cantidad_tienda_1}")

df.groupBy("tienda").count().show()
